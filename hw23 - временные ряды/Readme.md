#Итог
После проб многих вариантов гиперпараметров у CNN и Dense моделей, пришел к выводу, что показатели улучшаются с уменьшением количества нейронов. При уменьшении до 50 модель Dense уже избавилась от автокорреляции, а так же показала улучшение результата предсказания. Снизив количество до 16 добился почти сравнимой точности с LSTM моделью.

Что удивило - CNN модель не удалось отстроить хотя бы близко к точности LSTM а так же избавиться от автокорреляции. В последней итерации отстройки CNN график предсказания выглядит почти прямым для дифференцированных значений, а для не дифференцированных - он примерно повторяет изгибы истинных колебаний цены.

Тем не менее дифференцирование было выполнено, а так же выполнены обратные преобразования для получения исходных размеров цен, хоть это и не помогло в обучении CNN модели.

Как итог: выполнены задачи по подписи оси Х временными метками; в генераторе использован пакет, не равный 1; избавился от автокорреляции на графике Dense модели, а так же подстроил модель для повышения точности по сравнению с базовой урока 23.2; провел дифференцирование сета для избавления от тренда, а так же дополнил функции обратными преобразованиями при извлечении предсказанных данных.

6 /6

11.05.2025 04:21

Приветствую, Александр!

Общее впечатление:Вы проделали очень качественную и детальную работу! Видно, что Вы глубоко погрузились в тему временных рядов, экспериментировали с разными архитектурами нейронных сетей и тщательно анализировали результаты. Ваш подход к задаче систематичен, а отчет хорошо структурирован — это значительно облегчает проверку и понимание хода ваших мыслей.

Сильные стороны работы:

Полнота выполнения:

Вы выполнили все основные и дополнительные задания, включая работу с batch_size ≠ 1, дифференцирование датасета, отображение реальных дат на графиках и подбор архитектур для улучшения автокорреляции.

Отличная визуализация: графики обучения, предсказаний и автокорреляции оформлены четко и информативно.

Глубокий анализ:

Вы не просто применили готовые модели, но и провели эксперименты с гиперпараметрами (например, уменьшение числа нейронов в Dense-сети), что привело к улучшению результатов.

Хорошее понимание влияния дифференцирования на автокорреляцию и последующее восстановление исходного масштаба данных.

Работа с данными:

Корректная предобработка (нормировка, дифференцирование, восстановление цен после предсказания).

Правильная настройка генераторов временных рядов с учетом batch_size.

Критическое осмысление:

Вы честно отметили, что CNN-модель не показала ожидаемых результатов, и объяснили возможные причины. Это ценно для дальнейшего обучения.

Рекомендации по улучшению:

Оптимизация CNN:

Возможно, стоило попробовать другие параметры сверточных слоев (размер ядра, количество фильтров) или добавить BatchNormalization/Dropout для стабилизации обучения.

Эксперименты с stateful=True в LSTM.

Детализация выводов:

Хорошо бы еще кратко резюмировать, какие именно параметры (например, batch_size=2, Dense(16)) дали наилучший результат и почему.

Дополнительные метрики:

Кроме MSE и визуального анализа, можно добавить MAE или R² для количественной оценки качества прогноза.

Про баллы:

3 балла — базовая часть (обучение LSTM, графики, работа с batch_size).

+1 балл — использование Dense/CNN и улучшение автокорреляции.

+1 балл — дифференцирование датасета.

+1 балл — отображение реальных дат на графиках.

Итоговый балл: 6/6 (максимум за основное и дополнительные задания).

Отличная работа, Александр! Видно, что Вы не просто формально выполнили задания, а действительно разобрались в теме, проведя целое исследование. Такой подход обязательно приведет Вас к успеху в анализе временных рядов и deep learning. Продолжайте в том же духе!
